<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Z-Splat: Z-Axis Gaussian Splatting for Camera-Sonar Fusion</title>
  <link rel="icon" type="image/x-icon" href="static/images/icon.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Z-Splat: Z-Axis Gaussian Splatting for Camera-Sonar Fusion</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
              <a href="http://quintonq.top/index.php/projects/" target="_blank">Ziyuan Qu</a><sup>1</sup>,
              </span>
              <span class="author-block">
              <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Omkar Vengurlekar</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://www.cs.cmu.edu/~mqadri/" target="_blank">Mohamad Qadri</a><sup>3</sup>,
              </span>
              <span class="author-block">
                <a href="https://kevinwzhang.com/" target="_blank">Kevin Zhang</a><sup>4</sup>,
              </span>
              <span class="author-block">
                <a href="https://www.cs.cmu.edu/~kaess/" target="_blank">Michael Kaess</a><sup>3</sup>,
              </span>
              <span class="author-block">
                <a href="https://www.cs.umd.edu/~metzler/" target="_blank">Christopher Metzler</a><sup>4</sup>,
              </span>
              <span class="author-block">
                <a href="https://search.asu.edu/profile/3183646" target="_blank">Suren Jayasuriya</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://sites.google.com/view/adithyapediredla/" target="_blank">Adithya Pediredla</a><sup>1</sup>
              </span>
              </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Dartmouth College<sup>1</sup>, Arizona State University<sup>2</sup>, Carnegie Mellon University<sup>3</sup>, University of Maryland<sup>4</sup>
                      <br>ICCP 2024 (TPAMI Special Issue)</span>
                    <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2404.04687.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/QuintonQu/gaussian-splatting-with-depth/tree/gs-depth-main" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <span class="link-block">
                  <a href="https://drive.google.com/drive/folders/1R1uYa-h4ksr5HuNJ80vgUIKz1qGqF3JV?usp=drive_link" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fa fa-database"></i>
                  </span>
                  <span>Data</span>
                </a>
              </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2404.04687" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <!-- Your video here -->
        <source src="static/videos/ICCP-1min-video-z-splat-41.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Watch this 1-minute video for a quick overview of our Z-splat algorithm, including its purpose and a brief introduction. Check out the details below for more information.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Differentiable 3D-Gaussian splatting (GS) is emerging as a prominent technique in computer vision and graphics for reconstructing 3D scenes. GS represents a scene as a set of 3D Gaussians with varying opacities and employs a computationally efficient splatting operation along with analytical derivatives to compute the 3D Gaussian parameters given scene images captured from various viewpoints. Unfortunately, capturing surround view (360ยบ viewpoint) images is impossible or impractical in many real-world imaging scenarios, including underwater imaging, rooms inside a building, and autonomous navigation. In these restricted baseline imaging scenarios, the GS algorithm suffers from a well-known "missing cone" problem, which results in poor reconstruction along the depth axis. In this paper, we demonstrate that using transient data (from sonars) allows us to address the missing cone problem by sampling high-frequency data along the depth axis. We extend the Gaussian splatting algorithms for two commonly used sonars and propose fusion algorithms that simultaneously utilize RGB camera data and sonar data. Through simulations, emulations, and hardware experiments across various imaging scenarios, we show that the proposed fusion algorithms lead to significantly better novel view synthesis (5 dB improvement in PSNR) and 3D geometry reconstruction (60% lower Chamfer distance).
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content has-text-justified">
          <div class="level-set has-text-justified">
            <p>
              In this paper, we extend Gaussian splatting for sonar and build fusion techniques that reconstruct geometry using the complementary information from both the cameras and sonars. Our extension involves the development of splatting operations along the z-axis tailored for these sensor types.
              The specific contributions of this paper include:
             <ol>
              <li>A novel forward model to render the transient of Gaussian point clouds for two types of sonars: Echosounder and Forward-Looking Sonar (FLS).</li>
              <li>Fusion algorithms for cameras and sonars.</li>
              <li>Validation on synthetic, emulated hardware, and real hardware datasets showing that fusion Gaussian splatting results in better geometric (60%) and photometric (5 dB) reconstruction than standard camera-only Gaussian splatting.</li>
            </ol>
          </p>
        </div>
        <img src="static/images/01_splatting.png" alt="MY ALT TEXT" class="center-image blend-img-background "/>
        <p><strong>Ray View Transformation and Z-Axis Splatting</strong> (a) This illustration shows the camera view, which transforms the Gaussians from the world view to the camera view. (b) The Gaussians are transformed into the ray view through an local affine approximation of the projection transform using the Jacobian (J). (c) The transformed 3D Gaussian is then projected (splat) onto the xy-plane for rendering camera and z-axis for rendering echosounder (for collocated camera and echosounder). The gray Gaussian is occluded by the Gaussian in the front, so the Transmission(T) of that Gaussian is smaller than the others independent of whether we are rendering camera or sonar. Each ray undergoes splatting independently, ensuring that if a Gaussian is rasterized by multiple rays, it will be splatted multiple times. </p> 
        </div>
      </div>
    </div>
  </div>
</div>
</section>

<section class="hero is-light">
  <div class="hero-body">
    <div class="container  is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full">
      <h2 class="title is-3">Purpose: Small baseline and Missing Cone Problem</h2>
      <div class="level-set has-text-justified">
        <p>
          In small baseline imaging scenarios, camera images fail to capture depth-axis covariances and variance, resulting in missing cone problems in Fourier space. This limits 3D reconstruction fidelity due to missing frequency information. Our approach leverages time-resolved measurements from sonar to capture the z-axis projection of the volume, addressing the missing cone issue. By combining sonar data with camera images, we enhance 3D reconstruction, particularly in scenarios with limited camera baselines.
        </p>
     </div>
    </div>
  </div>
  <div class="columns is-centered">
     <img src="static/images/00_fusion_rationale_2.png" alt="MY ALT TEXT" class="center-image blend-img-background" width="60%"/>
  </div>
     <div class="columns is-centered">
      <div class="level-set has-text-justified">
          <p><strong>Sonar measurements provide complementary information.</strong> (a) Volumetric scene captured with three pairs of cameras and sonars (echosounder). We assume the sensors are in the far field (i.e., the local affine approximation to the projective transform in Gaussian splatting research is valid). For the center camera-sonar pair, camera measure- ments are obtained by projecting the volumetric data along the vertical axis, and sonar measurements are obtained by projecting the volumetric data along the horizontal axis. (b) If only camera measurements are considered, then using the Fourier-slice theorem, we are capturing only a few slices of the Fourier transform of the volume and missing information on a large cone. (c) Sonar (time-resolved data) captures orthogonal slices in the Fourier space, and hence, 3D reconstruction of the scene is better conditioned if we do the camera-sensor fusion instead of using only camera data. </p> 
      </div>
  </div>
  </div>
 </div>
</section>

<section class="hero is-small">
  <div class="hero-body">
    <div class="container  is-max-desktop">
      <h2 class="title is-3">Experiment Pipeline</h2>
      <div class="level-set has-text-justified">
     </div>
     <img src="static/images/02_simulation_process_echos.png" alt="MY ALT TEXT" class="center-image blend-img-background "/>
     <div class="level-set has-text-justified">
          <p><strong> Simulation and emulation pipeline for both echosounder and FLS fusion techniques </strong> (a) Raw depth image captured with Time-of-Flight (ToF) camera. (b) An RGB image captured with a camera. (c) Simulated echosounder intensity was generated using the depth histogram and utilized as ground truth during training. (d) A 3D Gaussian scene. We use xy-splatting to render RGB images and z-splatting to render echosounder depth intensity distribution. (e) Simulated FLS intensity generated by histogramming depth per row. (f) A 3D Gaussian scene, and we splat along xy-direction to render RGB image and along yz-direction to render FLS image. We minimize the sum of RGB loss and corresponding depth loss to train the camera-sonar fusion algorithms. </p>
     </div>
      </div>
   </div>
 </div>
</section>

<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Experiment Results</h2>
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/04_Simulation_Compare.png" alt="MY ALT TEXT" class="center-image blend-img-background"/>
        <h2 class="subtitle has-text-centered">
          <strong> Novel view synthesis comparison: </strong>  The incorporation of depth information notably mitigates the presence of floaters in the reconstructed scene. Moreover, depth information accurately positions the Gaussian kernels, particularly in scenes with uniform color or overexposure. 
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/06_Simulation_Compare_geo.png" alt="MY ALT TEXT" class="center-image blend-img-background"/>
        <h2 class="subtitle has-text-centered">
          <strong>  Geometry comparison on one-object scenes </strong> We captured the data by moving the camera only along the x-axis. We show ground truth meshes and superimpose the reconstructed Gaussians as point clouds. In the highlighted regions, we can observe that camera-only methods reconstruct the geometry inaccurately along the z-axis, whereas the proposed fusion techniques reconstruct the geometry accurately.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/07_emulation_1_cropped.png" alt="MY ALT TEXT" class="center-image blend-img-background"/>
        <h2 class="subtitle has-text-centered">
         <strong> Novel view synthesis comparison on emulated hardware </strong>  We set up a Cornell box in the lab, captured both RGB and depth images, and emulated the echosounder and FLS data. In the reconstructed scene, all methods work well on the objects with high-contrast textures. However, the RGB-only technique fails to reconstruct the white object with the same color as the background and also suffers from color bleeding. Our methods, on the other hand, successfully reconstruct the white object and do not suffer from color bleeding. For different random seeds, RGB-only techniques have high variance in the reconstructed results and have poor reconstructions (b).
       </h2>
     </div>
     <div class="item">
      <!-- Your image here -->
      <img src="static/images/09_echo_sounder_results - 01.png" alt="MY ALT TEXT" class="center-image blend-img-background"/>
      <h2 class="subtitle has-text-centered">
        <strong> Qualitative comparison of echosounder real-data results. </strong> The comparison presents two scenes captured using a DSLR camera and echo-sonar with a turntable setup. Our method demonstrates a noticeable improvement in performance over the baseline RGB-only method, both quantitatively and qualitatively.
      </h2>
    </div>
    <div class="item">
      <!-- Your image here -->
      <img src="static/images/FLS_results2 - 01.png" alt="MY ALT TEXT" class="center-image blend-img-background" width="40%"/>
      <h2 class="subtitle has-text-centered">
        <strong> Experimental reconstructions using RGB only and with FLS fusion. </strong> When using RGB-only measurements, we observe high error along depth (red box). Integrating FLS measurements improves depth resolution.
      </h2>
    </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->


<!-- Youtube video -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <!-- Paper video. -->
      <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="publication-video">
            <!-- Youtube embed code here -->
            <iframe src="https://www.youtube.com/embed/f4tFjl_Yz0I" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End youtube video -->


<!-- Paper poster -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/Z-Splat-ICCP-2024-small.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section>
<!--End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
      @article{qu2024z,
        title={Z-Splat: Z-Axis Gaussian Splatting for Camera-Sonar Fusion},
        author={Qu, Ziyuan and Vengurlekar, Omkar and Qadri, Mohamad and Zhang, Kevin and Kaess, Michael and Metzler, Christopher and Jayasuriya, Suren and Pediredla, Adithya},
        journal={arXiv preprint arXiv:2404.04687},
        year={2024}
      }</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the<a href="https://nerfies.github.io" target="_blank">Nerfies</a>project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
